{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9BpFGc9WZS2"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI8trZM2WZS7"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q92ZFeaWZS_"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        if self.name == 'cn':\n",
        "          for word in list(sentence):\n",
        "            self.addWord(word)\n",
        "        else:\n",
        "          for word in sentence.split(' '):\n",
        "              self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uW4VXDnWZTB"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! wget http://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz\n",
        "# ! gunzip news-commentary-v16.en-zh.tsv.gz"
      ],
      "metadata": {
        "id": "aa5hVewdXY09",
        "outputId": "ae40ccba-14fe-4543-d7b8-e9ee4459e18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-18 02:45:44--  http://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz [following]\n",
            "--2021-12-18 02:45:44--  https://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36938999 (35M) [application/x-gzip]\n",
            "Saving to: ‘news-commentary-v16.en-zh.tsv.gz’\n",
            "\n",
            "news-commentary-v16 100%[===================>]  35.23M  14.4MB/s    in 2.4s    \n",
            "\n",
            "2021-12-18 02:45:47 (14.4 MB/s) - ‘news-commentary-v16.en-zh.tsv.gz’ saved [36938999/36938999]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsOPnvEhWZTS"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('news-commentary-v16.en-zh.tsv').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9dGfIGCWZTW",
        "outputId": "0294bb1f-2f56-4567-a314-3df356f82c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 322274 sentence pairs\n",
            "Trimmed to 322274 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "cn 4726\n",
            "en 184192\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('en', 'cn', True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "id": "QQ8TJr58Y64X",
        "outputId": "1db5bd92-1301-4d17-9d35-41395aeb4724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['因此美国需求激增的快速增长加上强势美元可能有助于实现欧元区急需的（国与国之间经济）再平衡。', 'As a result, rapid demand-fueled growth in the US, together with the strong dollar, could contribute to a much-needed rebalancing of the eurozone.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2w8mydLWZTY"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMg3OWKFWZTd"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv2iNi3BWZTe"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    words = sentence.split(' ')\n",
        "    if lang.name == 'cn':\n",
        "      words = list(sentence)\n",
        "    return [lang.word2index[word] for word in words]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoOpPWikWZTg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaiGVi49WZTf"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L5XDN63WZTi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YQ-cPpTWZTh"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(pairs[i])\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cKznSFkWZTi"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu3w0aJFWZTj"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9KVp3LJWZTk"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainIters(encoder1, attn_decoder1, 75000, print_every=500)"
      ],
      "metadata": {
        "id": "EpgiZeZmc3J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3m 18s (- 493m 15s) (500 0%) 7.4923\n",
        "# 6m 29s (- 480m 48s) (1000 1%) 7.0536\n",
        "# 9m 34s (- 468m 58s) (1500 2%) 6.4656\n",
        "# 12m 34s (- 459m 7s) (2000 2%) 6.0801\n",
        "# 15m 47s (- 457m 57s) (2500 3%) 6.6461\n",
        "# 18m 42s (- 448m 56s) (3000 4%) 6.1082\n",
        "# 21m 39s (- 442m 30s) (3500 4%) 6.1037\n",
        "# 24m 51s (- 441m 7s) (4000 5%) 6.3101\n",
        "# 27m 58s (- 438m 13s) (4500 6%) 6.0242\n",
        "# 30m 56s (- 433m 17s) (5000 6%) 5.9279\n",
        "# 34m 4s (- 430m 35s) (5500 7%) 6.1502\n",
        "# 36m 59s (- 425m 21s) (6000 8%) 5.7882\n",
        "# 40m 3s (- 422m 11s) (6500 8%) 6.0404\n",
        "# 43m 17s (- 420m 31s) (7000 9%) 6.2131\n",
        "# 46m 33s (- 418m 58s) (7500 10%) 6.1599\n",
        "# 49m 32s (- 414m 58s) (8000 10%) 5.8316\n",
        "# 52m 45s (- 412m 42s) (8500 11%) 6.0925\n",
        "# 55m 44s (- 408m 45s) (9000 12%) 5.8976\n",
        "# 58m 49s (- 405m 36s) (9500 12%) 5.9933\n",
        "# 61m 33s (- 400m 5s) (10000 13%) 5.5543\n",
        "# 64m 32s (- 396m 30s) (10500 14%) 5.8250\n",
        "# 67m 33s (- 393m 3s) (11000 14%) 5.9574\n",
        "# 70m 46s (- 390m 48s) (11500 15%) 6.0898\n",
        "# 73m 59s (- 388m 25s) (12000 16%) 6.1450\n",
        "# 77m 26s (- 387m 10s) (12500 16%) 6.3143\n",
        "# 80m 41s (- 384m 48s) (13000 17%) 6.0470\n",
        "# 83m 57s (- 382m 27s) (13500 18%) 6.0484\n",
        "# 87m 10s (- 379m 51s) (14000 18%) 6.1230\n",
        "# 90m 25s (- 377m 19s) (14500 19%) 6.1979\n",
        "# 93m 41s (- 374m 45s) (15000 20%) 6.0666\n",
        "# 96m 58s (- 372m 17s) (15500 20%) 6.1216\n",
        "# 100m 21s (- 370m 2s) (16000 21%) 6.2585\n",
        "# 103m 50s (- 368m 8s) (16500 22%) 6.2053\n",
        "# 107m 19s (- 366m 8s) (17000 22%) 6.1113\n",
        "# 110m 48s (- 364m 3s) (17500 23%) 6.2310\n",
        "# 114m 16s (- 361m 53s) (18000 24%) 6.3253\n",
        "# 117m 37s (- 359m 13s) (18500 24%) 6.0541\n",
        "# 120m 55s (- 356m 24s) (19000 25%) 6.0778\n",
        "# 124m 20s (- 353m 54s) (19500 26%) 6.2580\n",
        "# 127m 50s (- 351m 33s) (20000 26%) 6.2468\n",
        "# 131m 13s (- 348m 52s) (20500 27%) 6.0498\n",
        "# 134m 37s (- 346m 11s) (21000 28%) 6.2316\n",
        "# 138m 2s (- 343m 30s) (21500 28%) 6.1163\n",
        "# 141m 24s (- 340m 39s) (22000 29%) 6.1240\n",
        "# 144m 44s (- 337m 42s) (22500 30%) 6.0711\n",
        "# 148m 9s (- 334m 57s) (23000 30%) 6.1579\n",
        "# 151m 36s (- 332m 15s) (23500 31%) 6.0588\n",
        "# 154m 57s (- 329m 17s) (24000 32%) 6.1983\n",
        "# 158m 28s (- 326m 40s) (24500 32%) 6.1283\n",
        "# 161m 51s (- 323m 42s) (25000 33%) 6.0781\n",
        "# 165m 27s (- 321m 11s) (25500 34%) 6.2563\n",
        "# 169m 3s (- 318m 37s) (26000 34%) 6.1817\n",
        "# 172m 44s (- 316m 8s) (26500 35%) 6.2616\n",
        "# 176m 17s (- 313m 24s) (27000 36%) 6.1437\n",
        "# 179m 44s (- 310m 27s) (27500 36%) 6.0741"
      ],
      "metadata": {
        "id": "W27azHkVGQeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def bleu_eval(encoder, decoder, iter, len):\n",
        "  curr_pairs = [pairs[i] for i in range(iter,iter+len)]\n",
        "  src = [c[0] for c in curr_pairs]\n",
        "  tgt = [[c[1].split()] for c in curr_pairs]\n",
        "  output = [evaluate(encoder, decoder, i)[0][:-1] for i in src]\n",
        "\n",
        "  print(\"tgt is\", tgt)\n",
        "  print(\"output is\", output)\n",
        "  bleu_score = corpus_bleu(tgt, output, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "  print(\"Input.     :\", src[0])\n",
        "  print(\"True output:\", tgt[0][0])\n",
        "  print(\"Output     :\", output[0])\n",
        "  print(\"BLEU Score = \", bleu_score)\n",
        "  print(\"-------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "cMaNg2ZNGgL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGdwdwUfWZTm"
      },
      "outputs": [],
      "source": [
        "bleu_eval(encoder1, attn_decoder1, 75000, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(75000, 75000 + 10):\n",
        "  pair = pairs[i]\n",
        "  print(\"src is\", pair[0])\n",
        "  print(\"tgt is\", pair[1])\n",
        "  print(\"output is\", ' '.join(evaluate(encoder1, attn_decoder1, pair[0])[0][:-1]) + '.')\n",
        "  print(\"_______________________\")"
      ],
      "metadata": {
        "id": "ZkP3ssCZRbe3",
        "outputId": "c998eeba-d99e-4f74-f28a-bd2f7756423a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src is 特朗普的竞选以好战的民族主义和移民提案为特色 — — 从大规模修墙阻挡移民到禁止所有穆斯林进入美国 — — 以及不分青红皂白地批评美国与盟国和敌国之间的关系。\n",
            "tgt is Trump’s campaign has been marked by bellicose nationalism and anti-immigrant proposals – from the construction of a massive wall to keep out immigrants to a ban on all Muslims from entering the US – as well as reckless criticisms of America’s relations with friends and foes alike.\n",
            "output is The – and and and and and and and and of and and and and and and and and of.\n",
            "_______________________\n",
            "src is 但他却仍有可能在选举中获胜；事实上，他能够指挥数量庞大且往往非常忠实的追随者。\n",
            "tgt is Yet he remains electorally viable; indeed, he commands a large and often virulently loyal following.\n",
            "output is But he is that a to a that to a to a that he is to.\n",
            "_______________________\n",
            "src is 其中反映并且强化的国内政治紧张凸显了今天的世界秩序是多么脆弱。\n",
            "tgt is The internal political strain that this reflects and reinforces highlights just how vulnerable the world order is today.\n",
            "output is The that is to a of and and and.\n",
            "_______________________\n",
            "src is 因为风格暴躁鲁莽，特朗普似乎缺乏当今世界稳定执政的所需的知识、智慧和性格。\n",
            "tgt is With his blustery and reckless style, Trump seems to lack the knowledge, wisdom, and temperament needed to execute the steady stewardship that today’s world requires.\n",
            "output is The and and and and and and and and and and and and and.\n",
            "_______________________\n",
            "src is 从这个意义上讲，他与乳臭未干的金正恩有很多共同点 — — 所不同的是，如果特朗普掌握权力，他所控制的将是影响力大得多的国家。\n",
            "tgt is In that sense, he has quite a lot in common with the callow Kim – except that, if Trump gains power, he will have control of a far more influential country.\n",
            "output is The a that a to a that of a to a that to a of a that to a a of.\n",
            "_______________________\n",
            "src is \n",
            "tgt is \n",
            "output is .\n",
            "_______________________\n",
            "src is 唐纳德·特朗普传递的信号\n",
            "tgt is Donald Trump’s Message\n",
            "output is The a a to.\n",
            "_______________________\n",
            "src is 坎布里奇 — — 唐纳德·特朗普11月在共和党总统候选人提名竞争中保持领先已经造成了某种程度的恐荒。\n",
            "tgt is CAMBRIDGE – Donald Trump’s lead in the race for the Republican Party’s nomination as its presidential candidate in November has caused consternation.\n",
            "output is The and and of and and and and and of and and and and and and of the.\n",
            "_______________________\n",
            "src is 共和党体制担心他无法战胜可能的民主党候选人希拉里·克林顿。\n",
            "tgt is The Republican establishment fears he will not be able to defeat Hillary Clinton, the likely Democratic nominee.\n",
            "output is The and and and and and and and.\n",
            "_______________________\n",
            "src is 但某些观察家则担心特朗普当选总统后美国的未来。\n",
            "tgt is But some observers worry more about the prospect of a Trump presidency.\n",
            "output is But that he is to a to a that to a a of.\n",
            "_______________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Jy5KWMNSV9E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "zh-en-machine-translation-baseline-rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}