{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM\n  \ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wget http://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz\n! gunzip news-commentary-v16.en-zh.tsv.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('news-commentary-v16.en-zh.tsv', sep='\\t', error_bad_lines=False, header=None).dropna().reset_index(drop=True)\ndf = df.sample(frac=1, random_state=1).reset_index(drop=True)\nprint(\"Len df is:\", len(df))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\nfrom typing import Iterable, List\nfrom transformers import AutoTokenizer\n\n\nSRC_LANGUAGE = 'chinese'\nTGT_LANGUAGE = 'english'\n\nLEN_DATA = len(df)\n\n# Place-holders\ntoken_transform = {SRC_LANGUAGE: None, TGT_LANGUAGE:None}\nvocab_transform = {SRC_LANGUAGE: None, TGT_LANGUAGE:None}\n\ndataset = [(ch,en) for ch, en in zip(list(df.iloc[:LEN_DATA][1]), list(df.iloc[:LEN_DATA][0]))]\n\n#dataset = {SRC_LANGUAGE: list(df.iloc[:len(df)][1]), TGT_LANGUAGE: list(df.iloc[:len(df)][0])}\n\ntoken_transform[SRC_LANGUAGE] = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\ntoken_transform[TGT_LANGUAGE] = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom typing import Tuple, List\ndef get_input_ids(lines: pd.core.series.Series, tokenizer: AutoTokenizer) -> List:\n    input_ids: List = []\n  \n    for line in lines:\n        line_ids = torch.tensor(tokenizer(line)['input_ids'], dtype=torch.long)\n\n        input_ids.append(line_ids)\n    return input_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom torchtext.legacy.datasets import Multi30k\nfrom torchtext.legacy.data import Field, BucketIterator\n\nimport spacy\nimport numpy as np\n\nimport random\nimport math\nimport time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set seed for reproducability\nSEED = 9120\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nimport math\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size) #division term 1/(10000^(2i/dim_model))\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)#embadding layer :vocab_size*emb_size\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 128,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size, \n                                       nhead=nhead, \n                                       num_encoder_layers=num_encoder_layers, \n                                       num_decoder_layers=num_decoder_layers, \n                                       dim_feedforward=dim_feedforward, \n                                       dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb,\n                                tgt_emb,\n                                src_mask,\n                                tgt_mask, \n                                None,\n                                src_padding_mask,\n                                tgt_padding_mask,\n                                memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(9120)\n\nSRC_VOCAB_SIZE = token_transform[SRC_LANGUAGE].vocab_size\nprint(SRC_VOCAB_SIZE)\nTGT_VOCAB_SIZE = token_transform[TGT_LANGUAGE].vocab_size\nprint(TGT_VOCAB_SIZE)\nEMB_SIZE = 256\nNHEAD = 8\nFFN_HID_DIM = 256\nBATCH_SIZE = 4\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nPAD_IDX = 0\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(DEVICE)\nprint(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\noptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001 * 5, betas=(0.9, 0.98), eps=1e-9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom torch.utils.data import DataLoader\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ntrain_index = len(dataset)*8//10\nprint(train_index)\nval_index = len(dataset)//10\nprint(val_index)\n\naccumulation_steps = 32\ndef train_epoch(model, optimizer):\n    index = 0\n    model.train()\n    losses = 0\n    train_dataloader = DataLoader(dataset[0:train_index], batch_size=BATCH_SIZE)# due to my gpu memory space i used first 100000 utterances to train\n    for src, tgt in train_dataloader:\n        src_tokens = get_input_ids(src, token_transform[SRC_LANGUAGE])\n        tgt_tokens = get_input_ids(tgt, token_transform[TGT_LANGUAGE])\n        \n        src_tokens = pad_sequence(src_tokens, padding_value=PAD_IDX)\n        tgt_tokens = pad_sequence(tgt_tokens, padding_value=PAD_IDX)\n\n        src_tokens = src_tokens.to(DEVICE)\n        tgt_tokens = tgt_tokens.to(DEVICE)\n\n        #teacher forcing \n        tgt_input = tgt_tokens[:-1, :] #the output embeddings are offset by one position\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_tokens, tgt_input)\n        \n        model(src_tokens, tgt_input)\n        logits = model(src_tokens, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n        optimizer.zero_grad()\n\n        tgt_out = tgt_tokens[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))#logits 3d to 2d tgt_out 2d to 1d logits total_number of words * vocal size tgt_out total_number of words\n        losses += loss.item()\n        loss = loss / accumulation_steps\n        loss.backward()\n        optimizer.step()\n\n        if (index+1) % accumulation_steps == 0:  \n          #print(loss.item())            # Wait for several backward steps\n          optimizer.step()                            # Now we can do an optimizer step\n          optimizer.zero_grad()    # Reset gradients tensors  \n\n        index += 1\n        if (index % 1000 == 0):\n          print(\"Current index:\", index)\n\n    return losses / len(train_dataloader)\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n    val_iter = dataset[train_index:train_index+val_index]\n    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE)\n\n    for src, tgt in val_dataloader:\n        src_tokens = get_input_ids(src, token_transform[SRC_LANGUAGE])\n        tgt_tokens = get_input_ids(tgt, token_transform[TGT_LANGUAGE])\n        \n        src_tokens = pad_sequence(src_tokens, padding_value=PAD_IDX)\n        tgt_tokens = pad_sequence(tgt_tokens, padding_value=PAD_IDX)\n\n        src_tokens = src_tokens.to(DEVICE)\n        tgt_tokens = tgt_tokens.to(DEVICE)\n\n        #teacher forcing \n        tgt_input = tgt_tokens[:-1, :] #the output embeddings are offset by one position\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_tokens, tgt_input)\n\n        logits = model(src_tokens, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt_tokens[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(val_dataloader)\n\nfrom timeit import default_timer as timer\nNUM_EPOCHS = 2\ntrain_record = []\nval_record = []\n\nprint(\"Start training\")\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    train_record.append(train_loss)\n    val_record.append(val_loss)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))    \n    torch.save(transformer.state_dict(), '/kaggle/working/zh_en_checkpoints') # save your trianed model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(transformer.state_dict(), '/kaggle/working/zh_en_checkpoints') # save your trianed model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BOS_IDX = 101\n\nEOS_IDX = 102\n\nMAX_LEN_TGT = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef bleu_eval(model,tokenizer, test_iter):\n  output = [translate(model, i[0]) for i in test_iter[:1000]]\n  true_output = [i[1] for i in test_iter[:1000]]\n  bleu_score = corpus_bleu(true_output, output, weights=(0.25, 0.25, 0.25, 0.25))\n\n  print(\"-------------------------------------------------------------\")\n  print(\"Input.     :\", [i[0] for i in test_iter[:1]])\n  print(\"True output:\", true_output[0])\n  print(\"Output     :\", output[0])\n  print(\"BLEU Score = \", bleu_score)\n  print(\"-------------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        #print(prob.shape)\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n        #print(next_word)\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\ndef sample_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        #print(prob.shape)\n        p = torch.nn.functional.softmax(prob/1.2)\n        #print(p.shape)\n        next_word = torch.multinomial(p.flatten(),1)[0]\n\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n# actual function to translate input sentence into target language\ndef translate(model: torch.nn.Module, src_sentence: str):\n    model.eval()\n    src = torch.tensor(token_transform[SRC_LANGUAGE](src_sentence)['input_ids']).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + MAX_LEN_TGT, start_symbol=BOS_IDX).flatten()\n    \n#     print(\"TGT is\", token_transform[TGT_LANGUAGE].decode(tgt_tokens))\n    return token_transform[TGT_LANGUAGE].decode(tgt_tokens).replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\ntranslate(transformer,'但这并没有对PD起到帮助作用。')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_iter = dataset[train_index+val_index:]\nbleu_eval(transformer,token_transform[TGT_LANGUAGE], test_iter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(transformer,'开展科学研究主要是为了改善提高我们的生活，但是，这也是一项产业，代表了来自政府和企业界的巨额投资。')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(transformer,'花费两千亿美元可以防止几十万人死亡，产生的效益高于成本25倍。')","metadata":{},"execution_count":null,"outputs":[]}]}