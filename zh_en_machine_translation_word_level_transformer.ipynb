{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd_nQ6mQgkEW"
      },
      "source": [
        "A part of the code is from this pytorch tutorial https://pytorch.org/tutorials/beginner/translation_transformer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3QmcUYBY2_l",
        "outputId": "fe003a16-f9ee-436d-d96e-17fc698da504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 7.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 665 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMJMjKqq_tl4",
        "outputId": "c8a80697-a034-4b26-ae16-3f1dc0d7bb24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.1.0\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (57.4.0)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (2.11.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 72.7 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.7\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (0.4.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.7\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.1.0) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.1 pydantic-1.8.2 spacy-3.1.0 spacy-legacy-3.0.8 srsly-2.4.2 thinc-8.0.13 typer-0.3.2\n",
            "Collecting zh-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.1.0/zh_core_web_sm-3.1.0-py3-none-any.whl (49.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.5 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-sm==3.1.0) (3.1.0)\n",
            "Collecting spacy-pkuseg<0.1.0,>=0.0.27\n",
            "  Downloading spacy_pkuseg-0.0.28-cp37-cp37m-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (0.6.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-sm==3.1.0) (0.29.24)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->zh-core-web-sm==3.1.0) (2.0.1)\n",
            "Installing collected packages: spacy-pkuseg, zh-core-web-sm\n",
            "Successfully installed spacy-pkuseg-0.0.28 zh-core-web-sm-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_sm')\n",
            "Collecting en-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.6 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.13)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy==3.1.0\n",
        "import spacy\n",
        "!python -m spacy download zh_core_web_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVgWHSm-ABO1",
        "outputId": "951402cf-958e-4332-8c1d-aa50aa812ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "%cd /mydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1A6rYg2lx3C",
        "outputId": "2b4ba0a4-6b26-4eec-8652-65e6c4005d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 18:36:16--  http://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36938999 (35M) [application/x-gzip]\n",
            "Saving to: ‘news-commentary-v16.en-zh.tsv.gz’\n",
            "\n",
            "news-commentary-v16 100%[===================>]  35.23M  47.9MB/s    in 0.7s    \n",
            "\n",
            "2021-12-10 18:36:17 (47.9 MB/s) - ‘news-commentary-v16.en-zh.tsv.gz’ saved [36938999/36938999]\n",
            "\n",
            "gzip: news-commentary-v16.en-zh.tsv already exists; do you wish to overwrite (y or n)? y\n"
          ]
        }
      ],
      "source": [
        "#! wget http://data.statmt.org/news-commentary/v16/training/news-commentary-v16.en-zh.tsv.gz\n",
        "#! gunzip news-commentary-v16.en-zh.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "I8uKysHZmt3m",
        "outputId": "fe902ff5-9334-4171-d0a6-9bb2d4c980fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndf = pd.read_csv(\\'news-commentary-v16.en-zh.tsv\\', sep=\\'\\t\\', error_bad_lines=False, header=None).dropna().reset_index(drop=True)\\ndf = df.sample(frac=1).reset_index(drop=True)\\nprint(\"Len df is:\", len(df))\\ndf.to_csv(\\'news-commentary-v16.en-zh.csv\\')\\ndf.head()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# load the data once, shuffle and then save it \n",
        "\"\"\"\n",
        "df = pd.read_csv('news-commentary-v16.en-zh.tsv', sep='\\t', error_bad_lines=False, header=None).dropna().reset_index(drop=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(\"Len df is:\", len(df))\n",
        "df.to_csv('news-commentary-v16.en-zh.csv')\n",
        "df.head()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('news-commentary-v16.en-zh.csv', error_bad_lines=False, header=None).dropna().reset_index(drop=True)\n",
        "df = df.drop(df.columns[[0]], axis=1).reset_index(drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cq8t-Mbb7N2R",
        "outputId": "3a8e8edb-a0e1-4b4f-9716-f067486e972d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Bolsheviks came to power on a promise of b...</td>\n",
              "      <td>布尔什维克靠着“面���会有的 ” （ 还有和平）的诺言获得了权力。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>And much of the anger that drove people to the...</td>\n",
              "      <td>而让人民涌向街头，让国家走向崩溃，让数百万人走出家园的愤怒背后正是对明晰的权利的渴望，包括保...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The pessimists claim that this is becoming har...</td>\n",
              "      <td>悲观主义者认为增长将变得更为困难和昂贵；乐观主义者则认为这一定律将随着芯片向3D阵列发展而继...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Russia and China today are united not only by ...</td>\n",
              "      <td>今天，将俄国和中国联系在一起的已经不仅仅是两国的能源协议，这两个国家都认定，属于他们的时代已...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet that isn’t helping the PD.</td>\n",
              "      <td>但这并没有对PD起到帮助作用。</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   1                                                  2\n",
              "0  The Bolsheviks came to power on a promise of b...                 布尔什维克靠着“面���会有的 ” （ 还有和平）的诺言获得了权力。\n",
              "1  And much of the anger that drove people to the...  而让人民涌向街头，让国家走向崩溃，让数百万人走出家园的愤怒背后正是对明晰的权利的渴望，包括保...\n",
              "2  The pessimists claim that this is becoming har...  悲观主义者认为增长将变得更为困难和昂贵；乐观主义者则认为这一定律将随着芯片向3D阵列发展而继...\n",
              "3  Russia and China today are united not only by ...  今天，将俄国和中国联系在一起的已经不仅仅是两国的能源协议，这两个国家都认定，属于他们的时代已...\n",
              "4                     Yet that isn’t helping the PD.                                    但这并没有对PD起到帮助作用。"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "W-kcFbTLpUgo",
        "outputId": "9134f458-0e6c-46b5-fa4d-8d8a804b899a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313674\n",
            "['布尔什维克靠着“面���会有的 ” （ 还有和平）的诺言获得了权力。', '而让人民涌向街头，让国家走向崩溃，让数百万人走出家园的愤怒背后正是对明晰的权利的渴望，包括保护财产的权利。', '悲观主义者认为增长将变得更为困难和昂贵；乐观主义者则认为这一定律将随着芯片向3D阵列发展而继续有效。', '今天，将俄国和中国联系在一起的已经不仅仅是两国的能源协议，这两个国家都认定，属于他们的时代已经到来。 世界需要他们更甚于他们需要世界，而美国尤其如此。', '但这并没有对PD起到帮助作用。', '为了解决这个问题，阿富汗教育部长哈尼夫·阿特马尔－可能是最具改革头脑的政府成员－正在寻求在阿富汗建立34所新的用于高等教育的宗教学校（他的目标是2,000所 ） 。', '试想如果阿拉伯人民能把自身从那种受侮辱受压迫者的心态中解脱出来，转变成一种充满希望的开放思维，那么就完全有可能接受以色列的存在。', '信仰也依然是许多美国人生命中的一个重要组成部分。', '而快速贬值则是央行的噩梦，因为这会造成通胀性后果。', '要保持竞争力，德国需要改变投资和创新税。']\n",
            "518\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy\n",
        "chinese = list(df.iloc[:len(df)][2])\n",
        "data = [len(i) for i in chinese]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "bins = np.arange(0, 500, 20) # fixed bin size\n",
        "\n",
        "plt.xlim([min(data)-5, max(data)+5])\n",
        "\n",
        "plt.hist(data, bins=bins, alpha=0.5)\n",
        "plt.axvline(sum(data)/len(data), color='k', linestyle='dashed', linewidth=1)\n",
        "plt.title('source (chinese) sentence length distirbution')\n",
        "plt.xlabel('sentence length (number of character)')\n",
        "plt.ylabel('count')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (1,10):\n",
        "  print(df[2][i])\n",
        "  print(df[1][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opDsxix63RnV",
        "outputId": "0c2f4f72-5329-4429-d783-f15881aa18e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And much of the anger that drove people to the streets, led countries to the point of collapse, and drove millions from their homes was motivated by a desire for clear rights, including those protecting property.\n",
            "而让人民涌向街头，让国家走向崩溃，让数百万人走出家园的愤怒背后正是对明晰的权利的渴望，包括保护财产的权利。\n",
            "The pessimists claim that this is becoming harder and more expensive; the optimists hold that the law will remain valid, with chips moving to three dimensions.\n",
            "悲观主义者认为增长将变得更为困难和昂贵；乐观主义者则认为这一定律将随着芯片向3D阵列发展而继续有效。\n",
            "Russia and China today are united not only by their energy deals, but also by both countries’ conviction that their time has come, and that the outside world needs them more than they need the outside world, particularly the US.\n",
            "今天，将俄国和中国联系在一起的已经不仅仅是两国的能源协议，这两个国家都认定，属于他们的时代已经到来。 世界需要他们更甚于他们需要世界，而美国尤其如此。\n",
            "Yet that isn’t helping the PD.\n",
            "但这并没有对PD起到帮助作用。\n",
            "To address this problem, Afghan Education Minister Hanif Atmar – perhaps the most reform-minded member of the government – is seeking to build 34 new madrassas (his goal is 2,000) in Afghanistan for higher learning.\n",
            "为了解决这个问题，阿富汗教育部长哈尼夫·阿特马尔－可能是最具改革头脑的政府成员－正在寻求在阿富汗建立34所新的用于高等教育的宗教学校（他的目标是2,000所 ） 。\n",
            "If Arab citizens could transform their culture of humiliation into a culture of hope, perhaps they would be able to reconcile themselves with Israel’s existence.\n",
            "试想如果阿拉伯人民能把自身从那种受侮辱受压迫者的心态中解脱出来，转变成一种充满希望的开放思维，那么就完全有可能接受以色列的存在。\n",
            "Faith remains for many in the United States a vital part of their lives.\n",
            "信仰也依然是许多美国人生命中的一个重要组成部分。\n",
            "And rapid depreciations are a central bank’s nightmare, given the inflationary consequences.\n",
            "而快速贬值则是央行的噩梦，因为这会造成通胀性后果。\n",
            "To remain competitive, Germany needs to change how it taxes investment and innovation.\n",
            "要保持竞争力，德国需要改变投资和创新税。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uky61argaUZ",
        "outputId": "1bab1b5b-2325-49d7-a87b-9638b216c582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250939\n",
            "31367\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Iterable, List\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "SRC_LANGUAGE = 'chinese'\n",
        "TGT_LANGUAGE = 'english'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {SRC_LANGUAGE: None, TGT_LANGUAGE:None}\n",
        "vocab_transform = {SRC_LANGUAGE: None, TGT_LANGUAGE:None}\n",
        "\n",
        "dataset = [(ch,en) for ch, en in zip(list(df.iloc[:len(df)][2]), list(df.iloc[:len(df)][1]))]\n",
        "train_index = len(dataset)*8//10\n",
        "print(train_index)\n",
        "val_index = len(dataset)//10\n",
        "print(val_index)\n",
        "training_dataset =  dataset[0:train_index]\n",
        "validation_dataset = dataset[train_index:train_index+val_index]\n",
        "\n",
        "#dataset = {SRC_LANGUAGE: list(df.iloc[:len(df)][1]), TGT_LANGUAGE: list(df.iloc[:len(df)][0])}\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='zh_core_web_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVENN4iXsrU8",
        "outputId": "272dd1fe-c30e-4dbb-c89d-0c57d9250523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13406447\n",
            "4723\n"
          ]
        }
      ],
      "source": [
        "# yields tuplet of english and french sentence\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1} \n",
        "    for data_sample in data_iter:\n",
        "        #print(token_transform[language](data_sample[language_index[language]]))\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "chinese = list(df.iloc[:len(df)][2])\n",
        "chinese_string = ''.join(elem for elem in chinese)\n",
        "print(len(chinese_string))\n",
        "print(len(set(chinese_string)))\n",
        "chinese_words = [i for i in yield_tokens(dataset, SRC_LANGUAGE)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_words = [i for i in yield_tokens(dataset, TGT_LANGUAGE)]"
      ],
      "metadata": {
        "id": "64Q0iwd7fvVb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuJKs4DBf4YN",
        "outputId": "897455d9-c5da-4ad3-8c02-e50d5047c4cb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'Bolsheviks', 'came', 'to', 'power', 'on', 'a', 'promise', 'of', 'bread', '(', 'and', 'peace', ')', '.'], ['And', 'much', 'of', 'the', 'anger', 'that', 'drove', 'people', 'to', 'the', 'streets', ',', 'led', 'countries', 'to', 'the', 'point', 'of', 'collapse', ',', 'and', 'drove', 'millions', 'from', 'their', 'homes', 'was', 'motivated', 'by', 'a', 'desire', 'for', 'clear', 'rights', ',', 'including', 'those', 'protecting', 'property', '.'], ['The', 'pessimists', 'claim', 'that', 'this', 'is', 'becoming', 'harder', 'and', 'more', 'expensive', ';', 'the', 'optimists', 'hold', 'that', 'the', 'law', 'will', 'remain', 'valid', ',', 'with', 'chips', 'moving', 'to', 'three', 'dimensions', '.'], ['Russia', 'and', 'China', 'today', 'are', 'united', 'not', 'only', 'by', 'their', 'energy', 'deals', ',', 'but', 'also', 'by', 'both', 'countries', '’', 'conviction', 'that', 'their', 'time', 'has', 'come', ',', 'and', 'that', 'the', 'outside', 'world', 'needs', 'them', 'more', 'than', 'they', 'need', 'the', 'outside', 'world', ',', 'particularly', 'the', 'US', '.'], ['Yet', 'that', 'is', 'n’t', 'helping', 'the', 'PD', '.'], ['To', 'address', 'this', 'problem', ',', 'Afghan', 'Education', 'Minister', 'Hanif', 'Atmar', '–', 'perhaps', 'the', 'most', 'reform', '-', 'minded', 'member', 'of', 'the', 'government', '–', 'is', 'seeking', 'to', 'build', '34', 'new', 'madrassas', '(', 'his', 'goal', 'is', '2,000', ')', 'in', 'Afghanistan', 'for', 'higher', 'learning', '.'], ['If', 'Arab', 'citizens', 'could', 'transform', 'their', 'culture', 'of', 'humiliation', 'into', 'a', 'culture', 'of', 'hope', ',', 'perhaps', 'they', 'would', 'be', 'able', 'to', 'reconcile', 'themselves', 'with', 'Israel', '’s', 'existence', '.'], ['Faith', 'remains', 'for', 'many', 'in', 'the', 'United', 'States', 'a', 'vital', 'part', 'of', 'their', 'lives', '.'], ['And', 'rapid', 'depreciations', 'are', 'a', 'central', 'bank', '’s', 'nightmare', ',', 'given', 'the', 'inflationary', 'consequences', '.'], ['To', 'remain', 'competitive', ',', 'Germany', 'needs', 'to', 'change', 'how', 'it', 'taxes', 'investment', 'and', 'innovation', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "data = [len(i) for i in english_words]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "bins = np.arange(0, numpy.max(data), 20) # fixed bin size\n",
        "plt.xlim([min(data)-5, max(data)+5])\n",
        "\n",
        "plt.hist(data, bins=bins, alpha=0.5)\n",
        "plt.axvline(sum(data)/len(data), color='k', linestyle='dashed', linewidth=1)\n",
        "plt.title(' sentence length distirbution')\n",
        "plt.xlabel('sentence length (number of words)')\n",
        "plt.ylabel('count')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LW3OEHZAff8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english = list(df.iloc[:len(df)][1])\n",
        "english_string = ''.join(elem for elem in english)\n",
        "print(len(set(english_string)))\n",
        "english_words = [i for i in yield_tokens(dataset, TGT_LANGUAGE)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi68YMnZ2LMr",
        "outputId": "aa4af70a-f255-48c9-a8a1-64ec5d118359"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#total number of words and number of unique words in english\n",
        "b = []\n",
        "for i in english_words:\n",
        "  b.extend(i)\n",
        "print(len(b))\n",
        "len(set(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwNtrdkk2zPS",
        "outputId": "ab190fb7-0667-4321-9ef3-a3336de1b035"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8080917\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70761"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of unique chinese word\n",
        "a = []\n",
        "for i in chinese_words:\n",
        "  a.extend(i)\n",
        "\n",
        "len(set(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe29f1sytfGE",
        "outputId": "9e437053-d555-4449-ad5c-f27b92dc2c23"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313674\n",
            "['布尔什维克', '靠着', '“', '面�', '��', '会', '有的', '”', '（', '还有', '和平', '）', '的', '诺言', '获得', '了', '权力', '。']\n",
            "7640775\n",
            "花费\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122432"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SL-9ccVsM1XH"
      },
      "outputs": [],
      "source": [
        "# yields tuplet of english and french sentence\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1} \n",
        "    for data_sample in data_iter:\n",
        "        #print(token_transform[language](data_sample[language_index[language]]))\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:#for each language\n",
        "    # Training data Iterator, contain tuplets of sentences\n",
        "    # Create torchtext's Vocab object which maps tokens to indices.\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(training_dataset, ln),#iterator – Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
        "                                                    min_freq=1,#min_freq – The minimum frequency needed to include a token in the vocabulary.\n",
        "                                                    specials=special_symbols,#specials – Special symbols to add. The order of supplied tokens will be preserved.\n",
        "                                                    special_first=True)#special_first – Indicates whether to insert symbols at the beginning or at the end of the vocab.\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAfDywwrvFU3",
        "outputId": "c7429900-4a5c-4348-fa8d-f471d5db3520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108342\n",
            "65583\n"
          ]
        }
      ],
      "source": [
        "#print vocab size\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  print(len(vocab_transform[ln]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BtYlInoTnZyh"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size) #division term 1/(10000^(2i/dim_model))\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)#embadding layer :vocab_size*emb_size\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, \n",
        "                                       nhead=nhead, \n",
        "                                       num_encoder_layers=num_encoder_layers, \n",
        "                                       num_decoder_layers=num_decoder_layers, \n",
        "                                       dim_feedforward=dim_feedforward, \n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb,\n",
        "                                tgt_emb,\n",
        "                                src_mask,\n",
        "                                tgt_mask, \n",
        "                                None,\n",
        "                                src_padding_mask,\n",
        "                                tgt_padding_mask,\n",
        "                                memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1afEgJjzn0EU"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"\n",
        "    prevent leftward information flow in the decoder to preserve the auto-regressive property. \n",
        "    \"\"\"\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)# source and target are padded with values PAD_IDX\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXGfOIVSn498",
        "outputId": "7c1293ef-e17b-46ae-abfc-830d42d74cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "108342\n",
            "65583\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "print(SRC_VOCAB_SIZE)\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "print(TGT_VOCAB_SIZE)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 4\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "print(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VKCIlgc-oIeD"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "#->BOS_IDX,token_ids,EOS_IDX\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))#Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    \n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "  \n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "      \n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX) #Pad a list of variable length Tensors with padding_value\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch.long() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsfCSqX456Lh",
        "outputId": "ad0478c6-012e-4d45-df25-e8ebe549e4bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (2): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (generator): Linear(in_features=512, out_features=65583, bias=True)\n",
              "  (src_tok_emb): TokenEmbedding(\n",
              "    (embedding): Embedding(108342, 512)\n",
              "  )\n",
              "  (tgt_tok_emb): TokenEmbedding(\n",
              "    (embedding): Embedding(65583, 512)\n",
              "  )\n",
              "  (positional_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# load last saved transformer\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "transformer.load_state_dict(torch.load('/mydrive/chinese/checkpoints'))\n",
        "transformer = transformer.to(DEVICE)\n",
        "transformer.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3jTpHHuoYON"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "  \n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "      \n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX) #Pad a list of variable length Tensors with padding_value\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch.long() \n",
        "\n",
        "accumulation_steps = 64\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    print(dataset[0])\n",
        "    train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)# due to my gpu memory space i used first 100000 utterances to train\n",
        "    print(len(train_dataloader))\n",
        "    for i,(src, tgt) in enumerate(train_dataloader):\n",
        "        #exit()\n",
        "        src = src.to(DEVICE)#sentence_length * batch size\n",
        "        tgt = tgt.to(DEVICE)\n",
        "        #teacher forcing \n",
        "        \n",
        "\n",
        "        tgt_input = tgt[:-1, :] #the output embeddings are offset by one position\n",
        "        \n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))#logits 3d to 2d tgt_out 2d to 1d logits total_number of words * vocal size tgt_out total_number of words\n",
        "        losses += loss.item()\n",
        "        loss = loss / accumulation_steps \n",
        "        loss.backward()\n",
        "\n",
        "        if (i+1) % accumulation_steps == 0:  \n",
        "          #print(loss.item())            # Wait for several backward steps\n",
        "          optimizer.step()                            # Now we can do an optimizer step\n",
        "          optimizer.zero_grad()    # Reset gradients tensors     \n",
        "        if i%1000 == 0:\n",
        "          print(i)\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 1\n",
        "train_record = []\n",
        "val_record = []\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    train_record.append(train_loss)\n",
        "    val_record.append(val_loss)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "#-----------------------------------512------------------------------    \n",
        "#Epoch: 1, Train loss: 5.993, Val loss: 4.897, Epoch time = 1380.780s\n",
        "#Epoch: 1, Train loss: 4.528, Val loss: 4.019, Epoch time = 1379.201s\n",
        "#Epoch: 1, Train loss: 3.825, Val loss: 3.472, Epoch time = 1382.013s\n",
        "#Epoch: 1, Train loss: 3.354, Val loss: 3.145, Epoch time = 1395.257s\n",
        "#Epoch: 1, Train loss: 3.028, Val loss: 2.929, Epoch time = 1391.281s\n",
        "#Epoch: 1, Train loss: 2.596, Val loss: 2.780, Epoch time = 1392.298s\n",
        "#------------------------------------256---------------------------------\n",
        "#Epoch: 1, Train loss: 6.610, Val loss: 5.508, Epoch time = 1282.349s\n",
        "#Epoch: 1, Train loss: 5.214, Val loss: 4.823, Epoch time = 1289.900s\n",
        "#Epoch: 1, Train loss: 4.672, Val loss: 4.362, Epoch time = 1294.279s\n",
        "#Epoch: 1, Train loss: 4.275, Val loss: 4.009, Epoch time = 1287.335s\n",
        "#Epoch: 1, Train loss: 3.949, Val loss: 3.714, Epoch time = 1289.114s\n",
        "#Epoch: 1, Train loss: 3.674, Val loss: 3.484, Epoch time = 1292.148s\n",
        "#Epoch: 1, Train loss: 3.446, Val loss: 3.306, Epoch time = 1286.921s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0vqD0vhuof2k"
      },
      "outputs": [],
      "source": [
        "#torch.save(transformer.state_dict(), '/mydrive/chinese_underfit/checkpoints') # save your trianed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wYADS6vweYO3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc92ed3a-ff67-4868-f473-e5c3eac23b0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The $ 100 million could prevent hundreds of thousands of deaths, with benefits higher benefits than the cost.'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "MAX_LEN_TGT = 64\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    #print(\"SRC\",src)\n",
        "    #print(\"SRC mask\",src_mask)\n",
        "    memory = model.encode(src, src_mask)\n",
        "    #print(\"memory\", memory)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        #print(prob.shape)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        #print(next_word)\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def sample_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        #print(prob.shape)\n",
        "        p = torch.nn.functional.softmax(prob/1.2)\n",
        "        #print(p.shape)\n",
        "        next_word = torch.multinomial(p.flatten(),1)[0]\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\" ,\",\",\").replace(\" .\",\".\").strip()\n",
        "    \n",
        "translate(transformer,'花费两千亿美元可以防止几十万人死亡，产生的效益高于成本25倍。')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWZ9iDRlABFN",
        "outputId": "72c20e04-7543-4457-c2d4-c02907a2fa94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('花费两千亿美元可以防止几十万人死亡，产生的效益高于成本25倍。', 'Spending $200 million could avert several hundred thousand deaths, yielding benefits that are 25 times higher than the costs.')\n",
            "31368\n"
          ]
        }
      ],
      "source": [
        "test_iter = dataset[train_index+val_index:]\n",
        "print(test_iter[-1])\n",
        "print(len(test_iter))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_iter.sort(key=lambda s: len(s[1]))"
      ],
      "metadata": {
        "id": "WAfAn4XdqNwo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqRx3YoQXVR",
        "outputId": "42c04fee-0fd8-43e7-c827-42e6df76b380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------\n",
            "True output: [[['The', 'opposition', ',', 'led', 'by', 'British', 'Prime', 'Minister', 'David', 'Cameron', 'with', 'the', 'support', 'of', '“', 'sovereignists', '”', 'across', 'Europe', ',', 'particularly', 'in', 'Scandinavia', ',', 'but', 'also', 'in', 'Hungary', ',', 'contends', 'that', 'someone', 'whom', 'the', 'majority', 'of', 'European', 'citizens', 'hardly', 'know', ',', 'can', 'not', 'claim', 'any', 'kind', 'of', 'political', 'legitimacy', '.']], [['Rather', 'than', 'offering', 'Band', '-', 'Aids', ',', 'the', 'ruling', 'coalition', 'should', 'be', 'working', 'to', 'boost', 'Italy', '’s', 'competitiveness', ',', 'which', 'has', 'been', 'undermined', 'since', 'the', 'crisis', 'by', 'the', 'weak', 'recovery', 'of', 'real', 'exports', 'and', 'low', 'investment', ',', 'which', ',', 'at', 'under', '9', '%', 'of', 'GDP', ',', 'remains', 'significantly', 'below', 'the', 'eurozone', 'average', '.']], [['To', 'that', 'end', ',', 'it', 'must', 'balance', 'its', 'imports', 'and', 'exports', ',', 'while', 'leveling', 'the', 'playing', 'field', 'for', 'foreign', 'corporations', 'operating', 'within', 'its', 'market', 'by', 'eliminating', 'the', 'incentives', 'for', 'local', 'governments', 'to', 'compete', 'for', 'FDI', 'regardless', 'of', 'cost', ',', 'or', 'to', 'engage', 'in', 'other', 'forms', 'of', 'undue', 'intervention', '.']], [['They', 'forced', 'the', 'Bundesbank', 'to', 'credit', 'the', 'purchase', 'of', 'goods', ',', 'services', ',', 'real', 'estate', ',', 'corporate', 'shares', ',', 'and', 'even', 'whole', 'companies', '–', 'or', 'at', 'least', 'to', 'credit', 'the', 'filling', 'of', 'bank', 'accounts', 'in', 'Germany', 'that', 'would', 'be', 'readily', 'available', 'for', 'asset', 'purchase', 'should', 'the', 'risk', 'of', 'a', 'euro', 'breakup', 'arise', '.']], [['To', 'kick', '-', 'start', 'that', 'process', ',', 'the', 'Global', 'Climate', 'Action', 'Summit', 'and', 'its', 'partners', 'have', 'issued', 'a', 'wide', 'array', 'of', 'new', 'challenges', ',', 'including', 'zero', '-', 'waste', 'goals', 'in', 'cities', ',', 'a', 'target', 'of', '500', 'companies', 'adopting', 'science', '-', 'based', 'targets', ',', 'and', 'initiatives', 'to', 'accelerate', 'uptake', 'of', 'zero', '-', 'emission', 'vehicles', '.']], [['Moreover', ',', 'by', 'communicating', 'ex', 'ante', 'its', 'willingness', 'to', 'provide', 'emergency', 'funds', ',', 'the', 'IMF', 'would', 'reduce', 'the', 'stigma', 'against', 'countries', 'that', 'turn', 'to', 'it', 'for', 'assistance', '–', 'a', 'stigma', 'that', 'has', 'led', 'countries', 'to', 'delay', 'requesting', 'support', 'until', 'it', 'is', 'too', 'late', 'to', 'mitigate', 'domestic', 'political', 'damage', '.']], [['This', 'certainly', 'was', 'a', 'step', 'in', 'the', 'right', 'direction', ',', 'but', 'it', 'did', 'nothing', 'to', 'address', 'the', 'foreign', '-', 'policy', 'failings', 'that', 'helped', 'spark', 'the', 'Ukrainian', 'crisis', 'and', 'continue', 'to', 'undermine', 'Europe', '’s', 'response', '–', 'namely', ',', 'the', 'EU', '’s', 'misguided', 'Neighborhood', 'Policy', '(', 'ENP', ')', 'and', 'its', 'muddled', 'approach', 'to', 'energy', '.']], [['Areas', 'of', 'focus', 'would', 'include', 'using', 'the', 'SDR', 'for', 'some', 'bond', 'issuance', 'and', 'trade', 'transactions', ',', 'developing', 'market', 'infrastructure', '(', 'including', 'payments', 'and', 'settlement', 'mechanisms', ')', ',', 'improving', 'valuation', 'methodologies', ',', 'and', 'gradually', 'developing', 'a', 'yield', 'curve', 'for', 'SDR', '-', 'denominated', 'loans', 'and', 'bonds', '.']], [['The', 'model', 'for', 'a', 'future', 'United', 'States', 'of', 'Europe', 'is', 'Switzerland', ',', 'a', 'country', 'with', 'four', 'languages', 'and', 'ethnicities', ',', 'fiscally', 'strong', 'sub', '-', 'national', 'units', '(', 'cantons', ')', ',', 'a', 'single', 'first', '-', 'rate', 'currency', ',', 'and', 'a', 'federal', 'government', 'and', 'a', 'parliament', 'that', 'exercises', 'genuine', ',', 'if', 'limited', ',', 'fiscal', 'authority', '.']], [['This', 'is', 'particularly', 'important', 'because', 'of', 'the', 'tendency', 'of', 'financial', 'markets', 'to', 'induce', 'short', '-', 'sighted', 'politicians', 'to', 'loosen', 'today', '’s', 'budget', 'constraints', ',', 'or', 'to', 'lend', 'to', 'flagrantly', 'corrupt', 'governments', 'such', 'as', 'the', 'fallen', 'Yanukovych', 'regime', 'in', 'Ukraine', ',', 'at', 'the', 'expense', 'of', 'future', 'generations', '.']]]\n",
            "Output     : [['The', 'opposition', ',', 'by', 'British', 'Prime', 'Minister', 'David', 'Cameron', ',', 'led', 'by', 'Prime', 'Minister', 'David', 'Cameron', ',', 'has', 'led', 'to', 'the', '“', 'pivot', '”', 'to', 'Europe', ',', 'and', 'the', 'support', 'of', 'most', 'European', 'citizens', ',', 'who', 'have', 'not', 'heard', 'of', 'any', 'political', 'legitimacy', '.'], ['With', 'the', 'policy', 'of', 'a', 'few', 'symptoms', ',', 'the', 'coalition', 'should', 'work', 'to', 'increase', 'Italy', '’s', 'competitiveness', 'and', 'slow', 'investment', 'levels', 'since', 'the', 'crisis', 'have', 'been', 'weakened', 'by', 'real', 'exports', '–', 'and', 'that', 'the', 'country', '’s', 'investment', 'ratio', 'has', 'not', 'been', 'below', '9', '%', ',', 'far', 'below', 'the', 'eurozone', '.'], ['And', 'it', 'must', 'achieve', 'balance', 'by', 'maintaining', 'its', 'own', 'trade', 'and', 'trade', ',', 'while', 'eliminating', 'incentives', 'for', 'local', 'governments', 'to', 'compete', 'for', 'foreign', 'direct', 'investment', 'or', 'implement', 'other', 'forms', 'of', 'interference', 'in', 'foreign', 'firms', 'to', 'provide', 'fair', 'competition', 'in', 'domestic', 'markets', '.'], ['They', 'forced', 'the', 'Bundesbank', 'to', 'buy', 'goods', ',', 'services', ',', 'real', 'estate', ',', 'and', 'even', 'the', 'entire', 'business', '-', 'sector', 'firms', '–', 'or', 'at', 'least', 'borrowing', '–', 'to', 'buy', 'the', 'risk', 'of', 'a', 'German', 'bank', ',', 'once', 'the', 'euro', 'collapse', 'could', 'be', 'used', 'to', 'buy', 'assets', '.'], ['To', 'promote', 'progress', ',', 'global', 'climate', 'action', ',', 'and', 'its', 'partners', 'have', 'issued', 'a', 'broad', 'list', 'of', 'new', 'challenges', ',', 'including', 'the', 'urban', 'zero', '-', 'zero', 'zero', '-', 'carbon', 'zero', '-', 'carbon', 'target', ',', 'the', '500', 'companies', 'adopt', 'scientific', 'evidence', '-', 'based', 'targets', ',', 'accelerating', 'zero', '-', 'carbon', 'emissions'], ['Moreover', ',', 'by', 'the', 'interim', 'plan', 'to', 'provide', 'emergency', 'funds', ',', 'the', 'IMF', 'could', 'reduce', 'the', 'humiliation', 'of', 'state', '-', 'led', 'humiliation', '–', 'a', 'combination', 'of', 'the', 'likes', 'of', 'the', 'likes', 'of', 'the', 'likes', 'of', 'the', 'state', ',', 'which', 'leaves', 'countries', 'with', 'no', 'less', 'likely', 'to', 'be', 'able', 'to', 'end'], ['This', 'is', 'undoubtedly', 'a', 'step', 'in', 'defense', 'for', 'defense', ',', 'but', 'it', 'is', 'not', 'to', 'address', 'the', 'failure', 'of', 'the', 'crisis', ',', 'undermining', 'the', 'failure', 'of', 'the', 'European', 'crisis', ',', 'which', 'is', 'the', 'European', 'Union', '’s', 'misguided', 'neighbors', ',', 'the', 'Alliance', ',', 'and', 'the', 'energy', 'approach', 'that', 'is', 'not', 'yet', 'to', 'do', '.'], ['The', 'focus', 'will', 'include', 'some', 'bonds', 'on', 'the', 'SDR', '-', 'free', 'and', 'trade', 'transactions', ',', 'including', 'payments', 'and', 'settlement', 'mechanisms', ',', 'improve', 'valuations', ',', 'and', 'improve', 'the', 'yield', 'curve', ',', 'and', 'gradually', 'the', 'yield', 'curve', 'of', 'the', 'SDR', '-', 'denominated', 'loans', 'and', 'bond', 'bonds', '.'], ['Switzerland', 'is', 'a', 'model', 'of', 'national', '-', 'level', 'model', 'that', 'is', 'the', 'entire', 'European', 'Union', ',', 'in', 'Switzerland', ',', 'where', 'four', 'languages', 'and', 'ethnic', 'units', 'of', 'the', 'state', '(', 'states', ')', 'are', 'strong', ',', 'and', 'the', 'new', 'unit', 'of', 'fiscal', 'forces', 'are', 'established', 'by', 'the', 'Swiss', 'unit', ',', 'the', 'entire', 'Swiss', 'government', ',', 'and', 'the', 'only', 'fiscal', 'rights', 'of', 'fiscal', 'rights', '.'], ['This', 'is', 'particularly', 'important', ',', 'because', 'financial', 'markets', 'tend', 'to', 'be', 'tempted', 'to', 'loosen', 'short', '-', 'sighted', 'budget', 'constraints', 'today', ',', 'or', 'borrowing', 'to', 'large', 'corruption', ',', 'such', 'as', 'Ukraine', '’s', 'Yanukovych', 'government', ',', 'thereby', 'reducing', 'the', 'offspring', 'of', 'the', 'population', '.']]\n",
            "BLEU Score =  0.14096469130451872\n",
            "-------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def bleu_eval(model,tokenizer, test_iter):\n",
        "  #--------------------------medium -----------------------------------------------------------\n",
        "  #output = [tokenizer(translate(model, i[0])) for i in test_iter[(len(test_iter)//2)-500:(len(test_iter)//2)+500]]\n",
        "  #true_output = [[tokenizer(i[1])] for i in test_iter[len(test_iter)//2-500:len(test_iter)//2+500]]\n",
        "  #--------------------------short----------------------------------------------------------------\n",
        "  output = [tokenizer(translate(model, i[0])) for i in test_iter[35:1035]]\n",
        "  true_output = [[tokenizer(i[1])] for i in test_iter[35:1035]]\n",
        "  #---------------------------long-----------------------------------------\n",
        "  output = [tokenizer(translate(model, i[0])) for i in test_iter[-1000:]]\n",
        "  true_output = [[tokenizer(i[1])] for i in test_iter[-1000:]]\n",
        "  \n",
        "  bleu_score = corpus_bleu(true_output, output, weights=(0.25, 0.25,0.25,0.25))\n",
        "\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "  print(\"True output:\", true_output[:10])\n",
        "  print(\"Output     :\", output[:10])\n",
        "  print(\"BLEU Score = \", bleu_score)\n",
        "  print(\"-------------------------------------------------------------\")\n",
        "bleu_eval(transformer,token_transform[TGT_LANGUAGE], test_iter)\n",
        "# BLEU Score = 0.1670790398116518\n",
        "# long sentences (58 english words on average) BLEU Score =  0.14096469130451872\n",
        "# short sentence (5 english words on average) BLEU Score =  0.13164372376705363\n",
        "# medium length sentences (25 english words on average) BLEU Score =  0.17301238771961439"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "print([i[0] for i in test_iter[:10]])\n",
        "#Epoch: 1, Train loss: 5.993, Val loss: 4.897, Epoch time = 1380.780s\n",
        "#Epoch: 1, Train loss: 4.528, Val loss: 4.019, Epoch time = 1379.201s\n",
        "#Epoch: 1, Train loss: 3.825, Val loss: 3.472, Epoch time = 1382.013s\n",
        "#Epoch: 1, Train loss: 3.354, Val loss: 3.145, Epoch time = 1395.257s\n",
        "#Epoch: 1, Train loss: 3.028, Val loss: 2.929, Epoch time = 1391.281s\n",
        "#Epoch: 1, Train loss: 2.596, Val loss: 2.780, Epoch time = 1392.298s\n",
        "plt.plot([5.993,4.528,3.825,3.354,3.028,2.596])\n",
        "plt.plot([4.897,4.019,3.472,3.145,2.929,2.780])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "NN-7zR7QYf21",
        "outputId": "58089f2f-5e9f-461c-a6b6-9a794837475e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['危机爆发前，欧洲似乎是首度成功实现政治一体化平衡状态可能性最大的候选人。', '尽管希腊救助计划已经完成，但欧元危机并未真正落幕，尤其是意大利可能成为风险的主要来源。', '“重启”与欧洲后院各国的关系', '没有理由表明包括债务上限在内的宪法性解决方案能够得到广泛的公众接受，尤其是债务国，它们因享乐无度的前任政府而经历了政治和经济伤害。', '治理委员会的许多人都反对这种幼稚的暗号游戏，因为一旦该委员会需要采取行动这种做法将会极大限制其自由。', '波士顿—8月，美国总统特朗普建议冻结汽车和卡车能源效率标准，环保主义者和他们的支持者为此怒不可遏。', '这些目标都早已有之。', '该党将可以自行组织政府，但是却缺少所需要的三分之二的多数来实行它所寻求的巨大变革。', '“因个人使用毒品，或拥有毒品以作个人使用而将人治罪侵犯了他们的自治权和隐私权。', '廉价的海地劳工已经取代了非技术型的多米尼加劳工，这在某种程度上扩大了收入不平等，降低了税收收入，致使国家公共财政和服务负担加重。']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_iter[35:100])\n",
        "true_output = [token_transform[TGT_LANGUAGE](i[1]) for i in test_iter[35:1035]]\n",
        "print(sum( map(len, true_output) ) / len(true_output))\n",
        "true_output = [token_transform[TGT_LANGUAGE](i[1]) for i in test_iter[len(test_iter)//2-500:len(test_iter)//2+500]]\n",
        "print(sum( map(len, true_output) ) / len(true_output))\n",
        "true_output = [token_transform[TGT_LANGUAGE](i[1]) for i in test_iter[-1000:]]\n",
        "print(sum( map(len, true_output) ) / len(true_output))\n",
        "\"\"\"\n",
        "print(sum( map(len, true_output) ) / len(true_output))\n",
        "true_output = [i[0] for i in test_iter[-1000:]]\n",
        "print(true_output[:10])\n",
        "print(sum( map(len, true_output) ) / len(true_output))\n",
        "true_output = [i[0] for i in test_iter[len(test_iter)//2-500:len(test_iter)//2+500]]\n",
        "print(sum( map(len, true_output) ) / len(true_output))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "mvaydC08qgS6",
        "outputId": "39153789-2330-4f04-c186-2ca97689694d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('也许如此。', 'Perhaps.'), ('这也许是对的。', 'Perhaps.'), ('我同意这一说法。', 'I agree.'), ('它确实重要。', 'It does.'), ('非也。', 'Well, no.'), ('完全不是。', 'Well, no.'), ('果真如此吗？', 'Are they?'), ('小心翼翼。', 'Gingerly.'), ('类似的情况还有很多。', 'And so on.'), ('普京经济学', 'Putinomics'), ('谁知道呢？', 'Who knows?'), ('恐怖主义？', 'Terrorism?'), ('但实际情况果真如此吗？', 'But is it?'), ('更多就业 ？ ”', 'More jobs?”'), ('明智的税收方式', 'Smart Taxes'), ('但是，事实果真如此吗？', 'But was it?'), ('美联储才是。', 'The Fed is.'), ('曼-德-拉 ！ ”', 'Man-del-a!”'), ('习近平在俄罗斯', 'Xi in Russia'), ('可怜的欧洲！', 'Poor Europe!'), ('这种说法我们不能同意。', 'We disagree.'), ('它真的活过来了 ！ ”', 'IT’S ALIVE!”'), ('其意思说得够明白了。', 'Enough said.'), ('他究竟是对是错？', 'Is he right?'), ('可能没有。', 'Perhaps not.'), ('果真如此吗？', 'Was it true?'), ('由谁来做呢？', 'And by whom?'), ('癌症世界', 'Cancer World'), ('请看下例。', 'Here is one.'), ('也许不是。', 'Perhaps not.'), ('一个新的特朗普？', 'A New Trump?'), ('3D幻想', '3D Fantasies'), ('实际情况甚至相去甚远。', 'Far from it.'), ('究竟发生了哪些变化？', 'What changed?'), ('卢拉走后又怎样', \"Lula's Legacy\"), ('我说 ， “ 这是为什么？', 'I said, “Why?'), ('其实真的不是。', 'Actually, no.'), ('我不知道。', 'I don’t know.'), ('啊天哪，是这样。', 'Oh lord, yes.'), ('印度的悲痛', 'India’s Agony'), ('但这一切都跟鱼类无关。', 'Not for fish.'), ('他错了。', 'He was wrong.'), ('但他从来未曾做到过。', 'He never did.'), ('虚拟的罪恶', 'Virtual Vices'), ('储蓄回报率当然没有那么高。', 'They are not.'), ('足球狂怒', 'Football Fury'), ('事实是否真的如此？', 'But are they?'), ('大概没有。', 'Possibly not.'), ('不要再重启', 'No More Resets'), ('因为事实就是如此。', 'Because it is.'), ('结果证明我们是对的。', 'We were right.'), ('瑞典之耻', 'Sweden’s Shame'), ('这真是疯狂。', 'That is crazy.'), ('但并非如此。', 'But it is not.'), ('泡沫的麻烦', 'Bubble Trouble'), ('储备改革', 'Reserve Reform'), ('几年时间过去了。', 'Years roll by.'), ('但是简而言之，取缔CDS是不正确的。', 'In a word, no.'), ('火炉之城', 'Furnace Cities'), ('公民贝索斯？', 'Citizen Bezos?'), ('为什么要畏怯呢？', 'Why should it?'), ('当然没有。', 'Of course not.'), ('拯救也门', 'Rescuing Yemen'), ('人种大杂烩', 'The Human Stew'), ('为了长高不要命？', 'Dying to Grow?')]\n",
            "5.006\n",
            "24.584\n",
            "57.509\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(sum( map(len, true_output) ) / len(true_output))\\ntrue_output = [i[0] for i in test_iter[-1000:]]\\nprint(true_output[:10])\\nprint(sum( map(len, true_output) ) / len(true_output))\\ntrue_output = [i[0] for i in test_iter[len(test_iter)//2-500:len(test_iter)//2+500]]\\nprint(sum( map(len, true_output) ) / len(true_output))\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEYEuItWma7X",
        "outputId": "bc002bb7-808e-4478-ac6f-652bbe230cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source sentence: 危机爆发前，欧洲似乎是首度成功实现政治一体化平衡状态可能性最大的候选人。\n",
            "target sentence: Before the crisis, Europe looked like the most likely candidate to make a successful transition to the first equilibrium – greater political unification.\n",
            "our translation: Before the crisis, Europe seems to be the most likely candidate for success in achieving political integration.\n",
            "source sentence: 尽管希腊救助计划已经完成，但欧元危机并未真正落幕，尤其是意大利可能成为风险的主要来源。\n",
            "target sentence: The euro crisis is not truly over, despite the completion of Greece’s bailout program, with Italy, in particular, representing a major source of risk.\n",
            "our translation: Although the Greek rescue program has been completed, the euro crisis did not end, especially Italy may become the main source of risk.\n",
            "source sentence: “重启”与欧洲后院各国的关系\n",
            "target sentence: A “Reset” Button for Europe’s Backyard\n",
            "our translation: “ reset ” relations with Europe ’s neighbors\n",
            "source sentence: 没有理由表明包括债务上限在内的宪法性解决方案能够得到广泛的公众接受，尤其是债务国，它们因享乐无度的前任政府而经历了政治和经济伤害。\n",
            "target sentence: There is no reason why a constitutional solution that involves debt limitation should not command a large measure of public acceptance, especially in debtor countries, which have experienced the political and economic damage caused by previous profligate governments.\n",
            "our translation: No reason why the coalition ’s constitutional resolution, including debt ceiling, can be widely accepted public acceptance, especially debtors, have experienced political and economic damage that the previous government was expected.\n",
            "source sentence: 治理委员会的许多人都反对这种幼稚的暗号游戏，因为一旦该委员会需要采取行动这种做法将会极大限制其自由。\n",
            "target sentence: Many on the Governing Council object to this childish game of code words, because it eliminates their freedom of action once the Council commits itself to a course of action.\n",
            "our translation: Many people who oppose the governance game, who are opposed to such a naive game, would be able to limit their freedom.\n",
            "source sentence: 波士顿—8月，美国总统特朗普建议冻结汽车和卡车能源效率标准，环保主义者和他们的支持者为此怒不可遏。\n",
            "target sentence: BOSTON – In August, when US President Donald Trump proposed to freeze fuel-efficiency standards for cars and trucks, environmentalists and their supporters were outraged.\n",
            "our translation: BOSTON – In August, US President Donald Trump proposed a freeze on cars and trucks, environmentalists and their supporters were so far behind.\n",
            "source sentence: 这些目标都早已有之。\n",
            "target sentence: There is nothing new about these objectives.\n",
            "our translation: These goals have long been.\n",
            "source sentence: 该党将可以自行组织政府，但是却缺少所需要的三分之二的多数来实行它所寻求的巨大变革。\n",
            "target sentence: The party will be able to form a government by itself, but will lack the two-thirds majority needed to bring about some of the big changes that it seeks.\n",
            "our translation: The party would be able to organize itself, but two - thirds of the most needed to implement it is seeking to implement it.\n",
            "source sentence: “因个人使用毒品，或拥有毒品以作个人使用而将人治罪侵犯了他们的自治权和隐私权。\n",
            "target sentence: “Subjecting people to criminal sanctions for the personal use of drugs, or for possession of drugs for personal use, infringes on their autonomy and right to privacy.\n",
            "our translation: “ For the use of drugs, or with drug drug use, the crime of drug is being accused of being accused of their autonomy and privacy.\n",
            "source sentence: 廉价的海地劳工已经取代了非技术型的多米尼加劳工，这在某种程度上扩大了收入不平等，降低了税收收入，致使国家公共财政和服务负担加重。\n",
            "target sentence: Cheap Haitian labor has become a substitute for less-skilled Dominican labor in a way that increases income inequalities, and puts a special burden on the country’s public finances and services, owing to lower tax revenues.\n",
            "our translation: The cheap Haiti has replaced non - tech labor labor, which has increased income inequality in some degree, reducing tax revenues, and reduced national public finances.\n"
          ]
        }
      ],
      "source": [
        "#translate the first 10 sentecnes\n",
        "for i in range (0,10):\n",
        "  print(\"source sentence:\", test_iter[i][0])\n",
        "  print(\"target sentence:\", test_iter[i][1])\n",
        "  print(\"our translation:\", translate(transformer,test_iter[i][0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(transformer,\"为了重获尊严，当选的政治人物必须显示出自身的权威性。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YnhdNe70_m5A",
        "outputId": "3650fe58-5537-40b0-a28f-7cf2fb59ca40"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'To be a dignity, elected political figures must show their own authority.'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "zh-en-machine-translation-word-level-transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}